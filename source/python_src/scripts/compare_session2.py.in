#!@PY_EXE@

#core imports
from __future__ import print_function
from __future__ import division
from builtins import str
from builtins import range
from past.utils import old_div
import argparse
import sys
import os
import math
import re
import copy

#non-core imports
#set the plotting back-end to 'agg' to avoid display
import matplotlib as mpl
# mpl.use('TkAgg')
import numpy as np
import scipy.stats
import pandas as pd
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

# import matplotlib.pyplot as plt
# import matplotlib.colors as mcolors
# import matplotlib.colorbar as mcbar
# import matplotlib.cm as cmx
# from matplotlib.ticker import MultipleLocator, FormatStrFormatter

#HOPS module imports
import vpal.processing
import vpal.utility
import vpal.fringe_file_manipulation

def col_nan_scatter(x,y, **kwargs):
    df = pd.DataFrame({'x':x[:],'y':y[:]})
    df = df.dropna()
    x = df['x']
    y = df['y']
    plt.gca()
    plt.scatter(x,y)

def col_nan_kde_histo(x, **kwargs):
    df = pd.DataFrame({'x':x[:]})
    df = df.dropna()
    x = df['x']
    plt.gca()
    plt.hist(x,bins=30)


def outlier_aware_hist(data, lower=None, upper=None, nbins='auto'):
    if not lower or lower < data.min():
        lower = data.min()
        lower_outliers = False
    else:
        lower_outliers = True

    if not upper or upper > data.max():
        upper = data.max()
        upper_outliers = False
    else:
        upper_outliers = True

    n, bins, patches = plt.hist(data, range=(lower, upper), bins=nbins)

    if lower_outliers:
        n_lower_outliers = (data < lower).sum()
        patches[0].set_height(patches[0].get_height() + n_lower_outliers)
        patches[0].set_facecolor('g')
        patches[0].set_label('Lower outliers: ({:1.1e}, {:1.1e})'.format(data.min(), lower))

    if upper_outliers:
        n_upper_outliers = (data > upper).sum()
        patches[-1].set_height(patches[-1].get_height() + n_upper_outliers)
        patches[-1].set_facecolor('r')
        patches[-1].set_label('Upper outliers: ({:1.1e}, {:1.1e})'.format(upper, data.max()))

    if lower_outliers or upper_outliers:
        plt.legend()


def mad(data):
    median = np.median(data)
    diff = np.abs(data - median)
    mad = np.median(diff)
    return mad

def calculate_bounds(data, z_thresh=3.5):
    #return (None,None)
    MAD = mad(data)
    median = np.median(data)
    const = 5*( z_thresh * MAD / 0.6745 )
    return (median - const, median + const)


#a not really robust way of dealing with sig-figs
def round_and_truncate(num, num_err):
    num_digits = 0
    err_exp = math.log10(num_err)
    if err_exp < 0:
        num_digits = abs( math.floor(err_exp) ) + 1
    a = round(num, int(num_digits) )
    b = round(num_err, int(num_digits) )
    return [ a , b  ]


#simple wrapper for linear function
class linear_fit_function(object):

    def __init__(self,origin=0):
        self.x0 = origin

    def linear_func(self,x, b, m):
        y = m*(x-self.x0) + b
        return y

def perform_least_squares_fit(x,y,yerr,names,mode=0):

    #first do a fit to determine what points may be outliers
    y_clean = []
    x_clean = []
    yerr_clean = []
    names_clean = []
    #quick empirical trim
    for n in list( range(0, len(x) ) ):
        if mode != 6:
            x_clean.append(x[n])
            y_clean.append(y[n])
            yerr_clean.append(yerr[n])
            names_clean.append(names[n])
        else:
            if abs(y[n]) < 140:
                x_clean.append(x[n])
                y_clean.append(y[n])
                yerr_clean.append(yerr[n])
                names_clean.append(names[n])
            # else:
            #     print("tossed, ", x[n], y[n],names[n])

    #offset
    x0 = x_clean[0];
    y_err = [1]*len(y_clean)


    #create fit function object
    fit_obj = linear_fit_function(x0)
    initial_param = [ np.mean(y_clean), 0.0 ] #offset, slope

    #do weighted least square regression
    wpopt, wpcov = curve_fit(fit_obj.linear_func, copy.copy(x_clean), copy.copy(y_clean), initial_param, sigma=copy.copy(y_err), absolute_sigma=False)
    y_fit = fit_obj.linear_func(x, *wpopt)
    y_res = y - y_fit

    #parameter errors, just take root of the diagonals of the covariance mx
    perr = np.sqrt(np.diag(wpcov))
    params = [wpopt[0], wpopt[1]]
    perr = [perr[0],  perr[1]]

    #x0, x, y, y-error, y-residuals, y-offset, y-fit, fit parameters, fit errors
    #return (x0, np.array(x), np.array(y), np.array(y_err),  np.array(y_fit), np.array(y_res), np.array(params), np.array(perr) )


    #now do the fit without the outliers
    x_trimmed = list()
    y_trimmed = list()
    yerr_trimmed = list()
    x_outlier = list()
    y_outlier = list()
    name_outlier = list()
    MAD = np.median(np.array( np.abs( y_res ) ) )

    for n in list( range(0, len(x) ) ):
        z = 0.6745*abs(y_res[n])/MAD
        if z < 20.0:
            x_trimmed.append(x[n])
            y_trimmed.append(y[n])
            yerr_trimmed.append(yerr[n])
        else:
            print("outlier:", x[n], y[n], y_res[n], ">", MAD, names[n])
            x_outlier.append(x[n])
            y_outlier.append(y[n])
            name_outlier.append(names[n])

    #offset
    x0 = x_trimmed[0];
    y_err = [1]*len(y_trimmed)

    #create fit function object
    fit_obj = linear_fit_function(x0)
    initial_param = [ np.mean(y_trimmed), 0.0 ] #offset, slope

    #do weighted least square regression
    wpopt, wpcov = curve_fit(fit_obj.linear_func, copy.copy(x_trimmed), copy.copy(y_trimmed), initial_param, sigma=copy.copy(y_err), absolute_sigma=False)
    y_fit = fit_obj.linear_func(x_trimmed, *wpopt)
    y_res = y_trimmed - y_fit

    #parameter errors, just take root of the diagonals of the covariance mx
    perr = np.sqrt(np.diag(wpcov))
    params = [wpopt[0], wpopt[1]]
    perr = [perr[0],  perr[1]]

    rmse = 0.0
    for yr in y_res:
        rmse += yr*yr
    rmse /= len(y_res)
    rmse = math.sqrt(rmse)
    print("RMSE = ", rmse)


    # # Number of observations, with count to not account for masked values
    # n_obs = yres.size
    # #n_obs = yres.count()
    #
    # # Degrees of freedom
    # dof = n_obs - npars - 1
    #
    # # Sum of weighted residuals
    # sumwr = np.sum( (yres * yres) / (yerr * yerr) )
    #
    # # Sum of weights
    # sumw  = np.sum( 1 / (yerr * yerr) )
    #
    # # Weighted rms
    # wrms  = np.sqrt( (n_obs / dof) * sumwr / sumw)
    #
    # # Chi2 per degree of freedom and normalized rms
    # chi2dof = sumwr / dof
    # nrms  = np.sqrt(chi2dof)
    #
    # # Return
    # return (wrms, nrms, n_obs, dof)


    # Sum of weighted residuals
    #$sumwr = np.sum( (yres * yres) / (yerr * yerr) )

    # Sum of weights
    #$sumw  = np.sum( 1 / (yerr * yerr) )

    # Weighted rms
    #wrms  = np.sqrt( (n_obs / dof) * sumwr / sumw)
    n_obs = len(y_res)
    #n_obs = yres.count()

    # Degrees of freedom
    dof = n_obs - 1

    wmean = 0.0
    wrms = 0.0
    sumw = 0.0
    for n in range(0,len(y_trimmed)):
        yr = y_trimmed[n]
        yre = yerr_trimmed[n]
        wrms += yr*yr/(yre*yre)
        sumw += 1./(yre*yre)
        wmean += yr/(yre*yre)
    #wrms /= len(y_res)
    #wrms = math.sqrt(wrms)

    wmean = wmean/sumw

    wrms = np.sqrt( (n_obs/dof)* wrms / sumw)
    print("WMRS = ",wrms)
    print("WMEAN = ", wmean)

    #x0, x, y, y-error, y-residuals, y-offset, y-fit, fit parameters, fit errors
    return (x0, np.array(x_trimmed), np.array(y_trimmed), np.array(yerr_trimmed), np.array(y_err),  np.array(y_fit), np.array(y_res), np.array(x_outlier), np.array(y_outlier), name_outlier, np.array(params), np.array(perr), rmse, wrms, wmean )


################################################################################
#constants
us_to_ps = 1e6


################################################################################
def main():

    parser = argparse.ArgumentParser(
        prog='compare_session_processing.py', \
        description='''compare differences between the processing of the same session in two different ways''' \
        )
    parser._action_groups.pop()
    required = parser.add_argument_group('required arguments')

    required.add_argument('-r', '--reference', dest='reference_file', help='the session summary file to use as reference', required=True)
    file_list_help_string = 'Space deliminated lists of session summary files to compare with the reference \n For example: ./bonn.csv ./waco.csv '
    required.add_argument("-l", "--list", dest='summary_files', nargs='*', help=file_list_help_string, required=True) #require argument with variable number
    required.add_argument("-m", "--mode", dest='mode', type=int, help= '0: snr vs. time\n' + \
        '1: snr delta hist\n'+ \
        '2: residual mbd delta hist\n'+ \
        '3: total mbd delta hist\n'+ \
        '4: dtec delta hist\n'+ \
        '5: residual mbd delta vs. time\n'+ \
        '6: total mbd delta vs. time\n', \
    required=True)

    args = parser.parse_args()
    all_files = dict()
    other_files = dict()
    mode = args.mode

    #construct dict of files to compare/process
    for x in args.summary_files:
        file_name = os.path.abspath(x)
        file_key = os.path.splitext( os.path.basename(file_name) )[0]
        other_files[file_key] = file_name
        all_files[file_key] = file_name
    other_keys = other_files.keys()

    #add the reference file
    reference_file_name = os.path.abspath(args.reference_file)
    reference_key = os.path.splitext( os.path.basename(reference_file_name) )[0]
    all_files[reference_key] = reference_file_name
    all_keys = all_files.keys()

    all_df = dict()

    #read in the reference file and sort by unique_key
    ref_df = pd.read_csv(all_files[reference_key], sep='\t')
    ref_df.sort_values(by=['unique_key'])
    all_df[reference_key] = ref_df

    #load other file info, these should all be dataframe compatible csv files
    #then sort by unique_key
    other_df = dict()
    for file_key in other_keys:
        other_df[file_key] = pd.read_csv(all_files[file_key], sep='\t')
        other_df[file_key].sort_values(by=['unique_key'])
        all_df[file_key] = other_df[file_key]

    #now we want to generate a set of unique_keys which are common among all files
    uk_set_list = list()
    for file_key in all_keys:
        print( all_df[file_key]['unique_key'] )
        uk_set_list.append( set( list(all_df[file_key]['unique_key']) ) )
        if file_key == reference_key:
            print("number of unique reference keys: ", len(all_df[file_key]['unique_key']) )

    #find intersection
    unique_key_set = set.intersection(*uk_set_list)

    print("number of overlapping unique keys: ", len(unique_key_set))

    #next move any unique_keys which are associated with an SNR lower than 10
    #in the reference data-set
    for index, row in ref_df.iterrows():
        uk = row['unique_key']
        snr = row['snr']
        if snr < 10.0:
            unique_key_set.remove(uk)
            print("removing data from: ", uk, " with snr: ", snr)

    #convert to list
    unique_keys = list(unique_key_set)

    #next thing we need to do filter out everything that is not in the unique_key set
    for file_key in all_keys:
        drop_idx = list()
        for index, row in all_df[file_key].iterrows():
            uk = row['unique_key']
            if uk not in unique_keys:
                drop_idx.append(index)
                print("will drop data from: ", uk, "for: ", file_key)
        #drop the relevant rows
        all_df[file_key].drop(drop_idx, inplace=True)
        all_df[file_key].sort_values(by=['unique_key'], inplace=True)

    # for file_key in all_keys:
    #     all_df[file_key] = all_df[file_key].filter(items=unique_keys, axis=0)
    #     all_df[file_key].sort_values(by=['unique_key'])

    #use the time tags from the reference df to select rows
    time_tag_keys = all_df[reference_key]['time_tag']

    #place to put bad scans
    bad_scan_list = set()

    #map files to plot styles...only good up to 8 files
    count = 0;
    marker_style_index = dict()
    marker_color = ['k','r','b','g', 'orange', 'c','m','y']
    marker_style = ['o', 'x', '+', '*', '^', 'v', 'h']
    for k in all_keys:
        marker_style_index[k] = count % len(marker_color)
        count += 1

    SMALL_SIZE = 10
    MEDIUM_SIZE = 18
    BIGGER_SIZE = 22
    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    # shqcode = all_df['shanghai']['quality']
    # print('shanghai = ', shqcode)

    #now do SNR comparison, we want plot of SNR vs time for each station_files on same plot
    if mode == 0:
        snr_vs_time = dict()
        qd_list = list()
        first_tk = np.min(time_tag_keys)
        fig = plt.figure(figsize=(10,6))
        ax1 = fig.add_subplot(111)
        for file_key in all_keys:
            times = list(all_df[file_key]['time_tag'])
            for n in list(range(0, len(times))):
                times[n] = times[n] - first_tk
            snr = all_df[file_key]['snr']
            snr_ref = all_df[reference_key]['snr']

            snr_perc_diff = []
            for n in range(0,len(snr)):
                snr_perc_diff.append( 100*abs( snr[n] - snr_ref[n])/snr_ref[n] )
            print(file_key)
            print("min, max, mean, median =", np.min(np.array(snr_perc_diff)), np.max(np.array(snr_perc_diff)) , np.mean(np.array(snr_perc_diff)), np.median(np.array(snr_perc_diff)) )
            print("--------------------")
            print(snr_perc_diff)
            ax1.scatter(times, snr, s=40, c=marker_color[ marker_style_index[file_key] ], marker=marker_style[ marker_style_index[file_key] ], label=file_key)
        ax1.set_title('SNR of each scan by correlator', fontsize=20)
        ax1.set_ylabel('SNR')
        ax1.set_xlabel('time since start (s)')
        plt.legend(loc='upper right');
        plt.show()

    if mode == 1:
        delta = dict()
        for file_key in all_keys:
            if file_key != reference_key:
                delta_list = list()
                ref = list(all_df[reference_key]['snr'])
                obj = list(all_df[file_key]['snr'])
                for n in list( range(0, min( len(ref), len(obj) ) ) ):
                    delta_val = ref[n] - obj[n]
                    delta_list.append(delta_val)
                delta[file_key] = delta_list
                fig = plt.figure(figsize=(8, 6))
                ax3 = fig.add_subplot(111)
                outlier_aware_hist( np.array(delta[file_key]), *calculate_bounds(np.array(delta[file_key]) ), nbins=100 )
                ax3.set_title('$\Delta$ SNR for: (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax3.set_ylabel('Number')
                ax3.set_xlabel('$\Delta$ SNR')
                plt.show()

    if mode == 2:
        delta = dict()
        for file_key in all_keys:
            if file_key != reference_key:
                delta_list = list()
                ref = list(all_df[reference_key]['mbdelay'])
                obj = list(all_df[file_key]['mbdelay'])
                for n in list( range(0, min( len(ref), len(obj) ) ) ):
                    delta_val = obj[n] - ref[n]
                    delta_list.append(delta_val*us_to_ps)
                delta[file_key] = delta_list
                fig = plt.figure(figsize=(8, 6))
                ax3 = fig.add_subplot(111)
                outlier_aware_hist( np.array(delta[file_key]), *calculate_bounds(np.array(delta[file_key]) ), nbins=100 )
                ax3.set_title('$\Delta$ residual MBD for : (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax3.set_ylabel('Number')
                ax3.set_xlabel('Residual MBD difference (ps)')
                plt.show()


    if mode == 3:
        delta = dict()
        for file_key in all_keys:
            if file_key != reference_key:
                delta_list = list()
                uk = list(all_df[reference_key]['unique_key'])
                other_uk = list(all_df[file_key]['unique_key'])
                sn = list(all_df[reference_key]['scan_name'])
                snr = list(all_df[reference_key]['snr'])
                ref = list(all_df[reference_key]['total_mbdelay'])
                obj =list(all_df[file_key]['total_mbdelay'])
                for n in list( range(0, min( len(ref), len(obj) ) ) ):
                    delta_val = obj[n] - ref[n]
                    delta_list.append(delta_val*us_to_ps)
                    if abs(delta_val*us_to_ps) > 100:
                        print(uk[n], other_uk[n])
                        print("large difference in total MBD: ", delta_val*us_to_ps, " ps, for: ", uk[n], " in: ", sn[n], " with snr: ", snr[n])
                delta[file_key] = delta_list
                fig = plt.figure(figsize=(8, 6))
                ax3 = fig.add_subplot(111)
                outlier_aware_hist( np.array(delta[file_key]), *calculate_bounds(np.array(delta[file_key]) ), nbins=100 )
                ax3.set_title('$\Delta$ total MBD for : (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax3.set_ylabel('Number')
                ax3.set_xlabel('Total MBD difference (ps)')
                plt.show()

    if mode == 4:
        delta = dict()
        for file_key in all_keys:
            if file_key != reference_key:
                delta_list = list()
                ref = list(all_df[reference_key]['dtec'])
                obj =list(all_df[file_key]['dtec'])
                for n in list( range(0, min( len(ref), len(obj) ) ) ):
                    delta_val = obj[n] - ref[n]
                    delta_list.append(delta_val)
                delta[file_key] = delta_list
                fig = plt.figure(figsize=(8, 6))
                ax3 = fig.add_subplot(111)
                outlier_aware_hist( np.array(delta[file_key]), *calculate_bounds(np.array(delta[file_key]) ), nbins=100 )
                ax3.set_title('$\Delta$ dTEC for : (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax3.set_ylabel('Number')
                ax3.set_xlabel('$\Delta$ dTEC (TEC units)')
                plt.show()

    if mode == 5:
        delta = dict()
        derr = dict()
        first_tk = np.min(time_tag_keys)
        for file_key in all_keys:
            if file_key != reference_key:
                times = list(all_df[file_key]['time_tag'])
                scans = list(all_df[file_key]['scan_name'])
                for n in list(range(0, len(times))):
                    times[n] = times[n] - first_tk
                delta_list = list()
                err_list = list()
                ref = list(all_df[reference_key]['mbdelay'])
                obj =list(all_df[file_key]['mbdelay'])
                obj_err = list(all_df[file_key]['mbd_error'])
                for n in list( range(0, min( len(ref), len(obj) ) ) ):
                    delta_val = obj[n] - ref[n]
                    err_val = obj_err[n]
                    delta_list.append(delta_val*us_to_ps)
                    err_list.append(err_val*us_to_ps)
                delta[file_key] = delta_list
                derr[file_key] = err_list
                fig = plt.figure(figsize=(8, 6))
                ax3 = fig.add_subplot(111)
                x0, x_trimmed, y_trimmed, y_err_trimmed, y_err, y_fit, y_res, x_out, y_out, name_out, param, perr, rmse, wrms, wmean = perform_least_squares_fit(times,delta[file_key],derr[file_key],scans,mode)
                for n in range(0,len(name_out)):
                    bad_scan_list.add(name_out[n])
                print(file_key, "outlier scans = ", name_out)
                #plot linear model
                plt.plot(x_trimmed, y_fit, linestyle='--', linewidth=1, color='blue')
                #plt.scatter(x=x_trimmed, y=y_trimmed, s=30, c='blue', marker='o' )
                plt.errorbar(x=x_trimmed, y=y_trimmed, yerr=y_err_trimmed,  markersize=7, fmt='o', color='b', )
                ax3.set_title('$\Delta$ residual MBD vs. time for : (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax3.set_ylabel('$\Delta$ residual MBD (ps)')
                plt.tick_params(top='off', right='on', which='both')
                ax3.set_xlabel('Time since start (s)')
                #add annotation
                rate, rate_err = round_and_truncate( param[1], perr[1] )
                offset, offset_err = round_and_truncate( param[0], perr[0] )
                ax3.annotate('Rate = ' + str(rate) + " $\pm$ " + str(rate_err) + " ps/s", xycoords='axes fraction', xy=(0.05, -0.2) )
                ax3.annotate('Offset = ' + str(offset) + " $\pm$ " + str(offset_err) + " ps", xycoords='axes fraction', xy=(0.05, -0.25) )
                ax3.annotate('RMSE = ' + str(round(rmse,2)) + " ps" , xycoords='axes fraction', xy=(0.05, -0.3) )
                ax3.annotate('WMEAN = ' + str(round(wmean,2)) + " ps" , xycoords='axes fraction', xy=(0.05, -0.34) )
                #ax3.annotate('WRMS = ' + str(round(wrms,2)) + " ps" , xycoords='axes fraction', xy=(0.05, -0.35) )
                #ax3.annotate('Outliers eliminated = ' + str(name_out), xycoords='axes fraction', xy=(0.05, -0.34) )
                fig.subplots_adjust(bottom=0.24)
                plt.show()


    if mode == 6:
        delta = dict()
        derr = dict()
        first_tk = np.min(time_tag_keys)
        for file_key in all_keys:
            if file_key != reference_key:
                times = list(all_df[file_key]['time_tag'])
                scans = list(all_df[file_key]['scan_name'])
                for n in list(range(0, len(times))):
                    times[n] = times[n] - first_tk
                delta_list = list()
                err_list = list()
                ref = list(all_df[reference_key]['total_mbdelay'])
                obj =list(all_df[file_key]['total_mbdelay'])
                obj_err = list(all_df[file_key]['mbd_error'])
                for n in list( range(0, min( len(ref), len(obj) ) ) ):
                    delta_val = obj[n] - ref[n]
                    err_val = obj_err[n]
                    delta_list.append(delta_val*us_to_ps)
                    err_list.append(err_val*us_to_ps)
                delta[file_key] = delta_list
                derr[file_key] = err_list
                fig = plt.figure(figsize=(8, 6))
                ax3 = fig.add_subplot(111)
                x0, x_trimmed, y_trimmed, y_err_trimmed, y_err, y_fit, y_res, x_out, y_out, name_out, param, perr, rmse, wrms, wmean = perform_least_squares_fit(times,delta[file_key],derr[file_key],scans,mode)
                for n in range(0,len(name_out)):
                    bad_scan_list.add(name_out[n])
                print(file_key, "outlier scans = ", name_out)
                #plot linear model
                mean_value = np.mean( np.array(y_trimmed) )
                y_fit = [mean_value]*len(x_trimmed)

                plt.plot(x_trimmed, y_fit, linestyle='--', linewidth=1, color='blue')
                plt.errorbar(x=x_trimmed, y=y_trimmed, yerr=y_err_trimmed,  markersize=7, fmt='o', color='r', )
                ax3.set_title('$\Delta$ total MBD vs. time for '+ file_key, fontsize=20)
                ax3.set_ylabel('$\Delta$ total MBD (ps)')
                #ax3.tick_params(labelright=True)
                plt.tick_params(top='off', right='on', which='both')
                ax3.set_xlabel('Time since start (s)')
                #add annotation
                rate, rate_err = round_and_truncate( param[1], perr[1] )
                offset, offset_err = round_and_truncate( param[0], perr[0] )
                ax3.annotate('Mean = ' + str(round( np.mean(np.array(y_trimmed) ), 2 ) ) + " ps", xycoords='axes fraction', xy=(0.05, -0.2) )
                ax3.annotate('Std. dev. = ' +  str(round( np.std(np.array(y_trimmed) ), 2 ) ) + " ps" , xycoords='axes fraction', xy=(0.05, -0.25) )
                ax3.annotate('WRMS = ' + str(round(wrms,2)) + " ps" , xycoords='axes fraction', xy=(0.05, -0.3) )
                #ax3.annotate('Outliers eliminated = ' + str(name_out), xycoords='axes fraction', xy=(0.05, -0.35) )
                ax3.annotate('WMEAN = ' + str(round(wmean,2)) + " ps" , xycoords='axes fraction', xy=(0.05, -0.35) )
                # ax3.annotate('Rate = ' + str(rate) + " $\pm$ " + str(rate_err) + " ps/s", xycoords='axes fraction', xy=(0.05, -0.2) )
                # ax3.annotate('Offset = ' + str(offset) + " $\pm$ " + str(offset_err) + " ps", xycoords='axes fraction', xy=(0.05, -0.25) )
                # ax3.annotate('RMSE = ' + str(round(rmse,2)) + " ps" , xycoords='axes fraction', xy=(0.05, -0.3) )
                # ax3.annotate('Outliers eliminated = ' + str(name_out), xycoords='axes fraction', xy=(0.05, -0.35) )
                fig.subplots_adjust(bottom=0.24)

                plt.show()

    for bd in bad_scan_list:
        print("===========================")
        print("===========================")
        for file_key in all_keys:
            bdthis = bd
            corr = file_key
            df = all_df[file_key]
            #print(bdthis, bdv, corr)
            bd_row = df.loc[ df['scan_name'] == bdthis ]
            if len(bd_row) != 0:
                print(corr, '&', bd , '&', bd_row.iloc[0]['source'], '&',  round(bd_row.iloc[0]['dtec'],2) , '&', \
                bd_row.iloc[0]['quality'], '&', round(bd_row.iloc[0]['snr'],2) , '&', bd_row.iloc[0]['total_mbdelay'])



if __name__ == '__main__':          # official entry point
    main()
    sys.exit(0)
