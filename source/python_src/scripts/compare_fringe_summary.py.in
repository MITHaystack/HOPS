#!@PY_EXE@

#core imports
import argparse
import sys
import os
import math
import re
import copy

#non-core imports
#set the plotting back-end to 'agg' to avoid display
import matplotlib as mpl
# mpl.use('TkAgg')
import numpy as np
import pandas as pd
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

#HOPS module imports
import vpal.processing
import vpal.utility
import vpal.fringe_file_manipulation


################################################################################
#constants/globals
us_to_ps = 1e6
x_coord = list()
y_coord = list()
unique_key_list = list()
################################################################################


def col_nan_scatter(x,y, **kwargs):
    df = pd.DataFrame({'x':x[:],'y':y[:]})
    df = df.dropna()
    x = df['x']
    y = df['y']
    plt.gca()
    plt.scatter(x,y)


def col_nan_kde_histo(x, **kwargs):
    df = pd.DataFrame({'x':x[:]})
    df = df.dropna()
    x = df['x']
    plt.gca()
    plt.hist(x,bins=30)


def outlier_aware_hist(data, lower=None, upper=None, nbins='auto'):
    if not lower or lower < data.min():
        lower = data.min()
        lower_outliers = False
    else:
        lower_outliers = True

    if not upper or upper > data.max():
        upper = data.max()
        upper_outliers = False
    else:
        upper_outliers = True

    n, bins, patches = plt.hist(data, range=(lower, upper), bins=nbins)

    if lower_outliers:
        n_lower_outliers = (data < lower).sum()
        patches[0].set_height(patches[0].get_height() + n_lower_outliers)
        patches[0].set_facecolor('g')
        patches[0].set_label('Lower outliers: ({:1.1e}, {:1.1e})'.format(data.min(), lower))

    if upper_outliers:
        n_upper_outliers = (data > upper).sum()
        patches[-1].set_height(patches[-1].get_height() + n_upper_outliers)
        patches[-1].set_facecolor('r')
        patches[-1].set_label('Upper outliers: ({:1.1e}, {:1.1e})'.format(upper, data.max()))

    if lower_outliers or upper_outliers:
        plt.legend()


def mad(data):
    median = np.median(data)
    diff = np.abs(data - median)
    mad = np.median(diff)
    return mad

def calculate_bounds(data, z_thresh=3.5):
    MAD = mad(data)
    median = np.median(data)
    const = 5*( z_thresh * MAD / 0.6745 )
    return (median - const, median + const)


#a not really robust way of dealing with sig-figs
def round_and_truncate(num, num_err):
    num_digits = 0
    err_exp = math.log10(num_err)
    if err_exp < 0:
        num_digits = abs( math.floor(err_exp) ) + 1
    a = round(num, int(num_digits) )
    b = round(num_err, int(num_digits) )
    return [ a , b  ]


#simple wrapper for linear function
class linear_fit_function(object):

    def __init__(self,origin=0):
        self.x0 = origin

    def linear_func(self,x, b, m):
        y = m*(x-self.x0) + b
        return y

def perform_least_squares_fit(x,y,yerr,names,mode=0):

    #first do a fit to determine what points may be outliers
    y_clean = []
    x_clean = []
    yerr_clean = []
    names_clean = []
    #quick empirical trim
    for n in list( range(0, len(x) ) ):
        if mode != 6:
            x_clean.append(x[n])
            y_clean.append(y[n])
            yerr_clean.append(yerr[n])
            names_clean.append(names[n])
        else:
            if abs(y[n]) < 140:
                x_clean.append(x[n])
                y_clean.append(y[n])
                yerr_clean.append(yerr[n])
                names_clean.append(names[n])
            # else:
            #     print("tossed, ", x[n], y[n],names[n])

    #offset
    x0 = x_clean[0];
    y_err = [1]*len(y_clean)


    #create fit function object
    fit_obj = linear_fit_function(x0)
    initial_param = [ np.mean(y_clean), 0.0 ] #offset, slope

    #do weighted least square regression
    wpopt, wpcov = curve_fit(fit_obj.linear_func, copy.copy(x_clean), copy.copy(y_clean), initial_param, sigma=copy.copy(y_err), absolute_sigma=False)
    y_fit = fit_obj.linear_func(x, *wpopt)
    y_res = y - y_fit

    #parameter errors, just take root of the diagonals of the covariance mx
    perr = np.sqrt(np.diag(wpcov))
    params = [wpopt[0], wpopt[1]]
    perr = [perr[0],  perr[1]]

    #x0, x, y, y-error, y-residuals, y-offset, y-fit, fit parameters, fit errors
    #return (x0, np.array(x), np.array(y), np.array(y_err),  np.array(y_fit), np.array(y_res), np.array(params), np.array(perr) )


    #now do the fit without the outliers
    x_trimmed = list()
    y_trimmed = list()
    yerr_trimmed = list()
    x_outlier = list()
    y_outlier = list()
    name_outlier = list()
    MAD = np.median(np.array( np.abs( y_res ) ) )

    for n in list( range(0, len(x) ) ):
        z = 0.6745*abs(y_res[n])/MAD
        if z < 20.0:
            x_trimmed.append(x[n])
            y_trimmed.append(y[n])
            yerr_trimmed.append(yerr[n])
        else:
            print("outlier:", x[n], y[n], y_res[n], ">", MAD, names[n])
            x_outlier.append(x[n])
            y_outlier.append(y[n])
            name_outlier.append(names[n])

    #offset
    x0 = x_trimmed[0];
    y_err = [1]*len(y_trimmed)

    #create fit function object
    fit_obj = linear_fit_function(x0)
    initial_param = [ np.mean(y_trimmed), 0.0 ] #offset, slope

    #do weighted least square regression
    wpopt, wpcov = curve_fit(fit_obj.linear_func, copy.copy(x_trimmed), copy.copy(y_trimmed), initial_param, sigma=copy.copy(y_err), absolute_sigma=False)
    y_fit = fit_obj.linear_func(x_trimmed, *wpopt)
    y_res = y_trimmed - y_fit

    #parameter errors, just take root of the diagonals of the covariance mx
    perr = np.sqrt(np.diag(wpcov))
    params = [wpopt[0], wpopt[1]]
    perr = [perr[0],  perr[1]]

    rmse = 0.0
    for yr in y_res:
        rmse += yr*yr
    rmse /= len(y_res)
    rmse = math.sqrt(rmse)
    print("RMSE = ", rmse)

    n_obs = len(y_res)
    # Degrees of freedom
    dof = n_obs - 1

    wmean = 0.0
    wrms = 0.0
    sumw = 0.0
    for n in range(0,len(y_trimmed)):
        yr = y_trimmed[n]
        yre = yerr_trimmed[n]
        wrms += yr*yr/(yre*yre)
        sumw += 1./(yre*yre)
        wmean += yr/(yre*yre)

    wmean = wmean/sumw

    wrms = np.sqrt( (n_obs/dof)* wrms / sumw)
    print("WMRS = ",wrms)
    print("WMEAN = ", wmean)

    #x0, x, y, y-error, y-residuals, y-offset, y-fit, fit parameters, fit errors
    return (x0, np.array(x_trimmed), np.array(y_trimmed), np.array(yerr_trimmed), np.array(y_err),  np.array(y_fit), np.array(y_res), np.array(x_outlier), np.array(y_outlier), name_outlier, np.array(params), np.array(perr), rmse, wrms, wmean )



def display_point_info(event):
    global x_coord
    global y_coord
    global unique_key_list
    
    if event.inaxes:
        x_click, y_click = event.xdata, event.ydata
        print(f"Clicked at coordinates: x={x_click:.6f}, y={y_click:.6f}")

        # Find the nearest data point
        distances = ((x_coord - x_click)**2 + (y_coord - y_click)**2)**0.5
        nearest_index = distances.argmin()

        print(f"Nearest data point: x={x_coord[nearest_index]:.6f}, y={y_coord[nearest_index]:.6f}")
        print(f"Unique key of nearest data point = ", unique_key_list[nearest_index])


################################################################################
def main():
    
    global x_coord
    global y_coord
    global unique_key_list

    parser = argparse.ArgumentParser(
        prog='compare_fringe_summary.py', \
        description='''utility to compare differences between the fringe summary files of the same session processed in two different ways''' \
        )
    parser._action_groups.pop()
    required = parser.add_argument_group('required arguments')

    required.add_argument('-r', '--reference', dest='reference_file', help='the session summary file to use as reference', required=True)
    file_list_help_string = 'Space delimited lists of session summary files to compare with the reference \n For example: ./bonn.csv ./waco.csv '
    required.add_argument("-l", "--list", dest='summary_files', nargs='*', help=file_list_help_string, required=True) #require argument with variable number

    #specify the data quantity of interest
    required.add_argument("-i", "--item", dest='item', type=str, help= 'the item to compare (e.g. snr, mbd), options are: \n' + \
        'snr, \n'+ \
        'amp, \n'+ \
        'mbd (residual),\n'+ \
        'sbd (residual),\n'+ \
        'delay_rate (residual),\n'+ \
        'dtec, \n'+ \
        'phase (residual), \n'+ \
        'mbd_error, \n'+ \
        'total_mbd, \n'+ \
        'total_sbd, \n'+ \
        'total_delay_rate, \n'+ \
        'total_phase, \n' + \
        'apriori_delay, \n' + \
        'apriori_rate, \n' + \
        'apriori_accel \n', \
    required=True)

    #specify the manner in which we should plot the data
    required.add_argument("-m", "--mode", dest='mode', type=int, help= \
        '0: plot histogram of the difference (limit outliers) \n' + \
        '1: plot reference value vs. other \n'+ \
        '2: plot difference vs time \n' + \
        '3: plot raw histogram of the difference', \
    required=True)

    parser.add_argument('-s', '--snr-min', type=float, dest='snr_min', help='set minimum allowed snr threshold, default=10.', default=10.0)
    parser.add_argument('-q', '--quality-limit', type=int, dest='quality_lower_limit', help='set the lower limit on fringe quality (inclusive), default=0.', default=0)

    args = parser.parse_args()
    all_files = dict()
    other_files = dict()
    mode = args.mode
    
    #construct a dictionary which maps to the quantity of interest to lookup key
    item2elem = dict()
    item2elem['snr'] = 'snr'
    item2elem['amp'] = 'amp'
    item2elem['mbd'] = 'mbdelay'
    item2elem['sbd'] = 'sbdelay'
    item2elem['delay_rate'] = 'delay_rate'
    item2elem['dtec'] = 'dtec'
    item2elem['phase'] = 'resid_phas'
    item2elem['mbd_error'] = 'mbd_error'
    item2elem['total_mbd'] = 'total_mbdelay'
    item2elem['total_sbd'] = 'total_sbresid'
    item2elem['total_delay_rate'] = 'total_rate'
    item2elem['total_phase'] = 'total_phas'
    item2elem['apriori_delay'] = 'adelay'
    item2elem['apriori_rate'] = 'arate'
    item2elem['apriori_accel'] = 'aaccel'
    element_name = "unknown"
    if args.item in item2elem:
        element_name = item2elem[args.item]
    
    #construct dict of files to compare/process
    for x in args.summary_files:
        file_name = os.path.abspath(x)
        file_key = os.path.splitext( os.path.basename(file_name) )[0]
        other_files[file_key] = file_name
        all_files[file_key] = file_name
    other_keys = other_files.keys()

    #add the reference file
    reference_file_name = os.path.abspath(args.reference_file)
    reference_key = os.path.splitext( os.path.basename(reference_file_name) )[0]
    all_files[reference_key] = reference_file_name
    all_keys = all_files.keys()

    #set up plot styles...only good up to 8 files
    count = 0;
    marker_style_index = dict()
    marker_color = ['k','r','b','g', 'orange', 'c','m','y']
    marker_style = ['o', 'x', '+', '*', '^', 'v', 'h']
    for k in all_keys:
        marker_style_index[k] = count % len(marker_color)
        count += 1
    SMALL_SIZE = 10
    MEDIUM_SIZE = 18
    BIGGER_SIZE = 22
    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
    

    all_df = dict()
    #read in the reference file and sort by unique_key
    ref_df = pd.read_csv(all_files[reference_key], sep=',')
    ref_df.sort_values(by=['unique_key'])
    all_df[reference_key] = ref_df

    print(ref_df)
    print(ref_df.columns)

    #load other file info, these should all be dataframe compatible csv files
    #then sort by unique_key
    other_df = dict()
    for file_key in other_keys:
        other_df[file_key] = pd.read_csv(all_files[file_key], sep=',')
        print(other_df[file_key].columns)
        print(other_df[file_key])
        other_df[file_key].sort_values(by=['unique_key'])
        all_df[file_key] = other_df[file_key]

    #now we want to generate a set of unique_keys which are common among all files
    uk_set_list = list()
    for file_key in all_keys:
        print( all_df[file_key]['unique_key'] )
        uk_set_list.append( set( list(all_df[file_key]['unique_key']) ) )
        if file_key == reference_key:
            print("number of unique reference keys: ", len(all_df[file_key]['unique_key']) )

    #find intersection
    unique_key_set = set.intersection(*uk_set_list)
    print("number of overlapping unique keys: ", len(unique_key_set))
    #next remove any unique_keys which are associated with an SNR lower than 10
    #in the reference data-set
    qlimit = args.quality_lower_limit
    snr_limit = args.snr_min
    for index, row in ref_df.iterrows():
        uk = row['unique_key']
        snr = row['snr']
        errcode = str(row['errcode']).strip()
        qcode = row['quality']
        if snr < snr_limit or qcode < qlimit or errcode != "":
            if uk in unique_key_set:
                unique_key_set.remove(uk)
                print("removing data from: ", uk, " with snr: ", snr, " errcode: ", errcode, " quality: ", qcode)

    #convert to list
    unique_keys = list(unique_key_set)
    #next thing we need to do is filter out data that is not in the unique_key set
    for file_key in all_keys:
        dropped_count = 0
        drop_idx = list()
        for index, row in all_df[file_key].iterrows():
            uk = row['unique_key']
            if uk not in unique_keys:
                drop_idx.append(index)
                dropped_count += 1
                #print("will drop data from: ", uk, "for: ", file_key)
        #drop the relevant rows
        all_df[file_key].drop(drop_idx, inplace=True)
        all_df[file_key].sort_values(by=['unique_key'], inplace=True)
        print("dropping ", dropped_count, " fringes from: ", file_key)
        
    #reduce the data frames to just the minimal set of values we want to plot
    reduced_data = dict()
    #initialize using the reference data 
    for uk in unique_keys:
        reduced_data[uk] = dict()
        #extract the row where'unique_key' == uk:
        df = all_df[reference_key]
        filtered_df = df[ df['unique_key'] == uk]
        #filtered_df.reset_index(drop=True)
        if not filtered_df.empty:
            reduced_data[uk][reference_key] = dict()
            #print("filtered df = ", filtered_df)
            #now extract the value:
            value = float( filtered_df[element_name].values[0] )
            ttime = str( filtered_df['time_tag'].values[0] )
            #print("v = ", value)
            #print("tt = ", ttime)
            reduced_data[uk][reference_key][element_name] = value
            reduced_data[uk][reference_key]['time_tag'] = ttime
            reduced_data[uk][reference_key]['delta'] = 0.0
    
    #now extract the other data and figure out the difference 
    for uk in unique_keys:
        for file_key in all_keys:
            if file_key != reference_key:
                #extract the row where'unique_key' == uk:
                df = all_df[file_key]
                filtered_df = df[ df['unique_key'] == uk]
                if not filtered_df.empty:
                    reduced_data[uk][file_key] = dict()
                    #print("filtered df = ", filtered_df)
                    #now extract the value:
                    value = float( filtered_df[element_name].values[0] )
                    ttime = str( filtered_df['time_tag'].values[0] )
                    #print("v = ", value)
                    #print("tt = ", ttime)
                    ref_value = reduced_data[uk][reference_key][element_name]
                    delta = value - ref_value
                    reduced_data[uk][file_key][element_name] = value 
                    reduced_data[uk][file_key]['time_tag'] = ttime
                    reduced_data[uk][file_key]['delta'] = delta

    
    #now go through and make the plots
    if mode == 0: #make a histogram
        for file_key in all_keys:
            if file_key != reference_key:
                delta_list = list()
                for uk in unique_keys:
                    delta = reduced_data[uk][file_key]['delta']
                    delta_list.append(delta)
                fig = plt.figure(figsize=(8, 6))
                ax= fig.add_subplot(111)
                outlier_aware_hist( np.array(delta_list), *calculate_bounds(np.array(delta_list) ), nbins=100 )
                ax.set_title('$\Delta$ ' + element_name + ' for: (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax.set_ylabel('Number')
                ax.set_xlabel('$\Delta$ ' + element_name)
                plt.show()

    if mode == 1: #make a scatter plot
        for file_key in all_keys:
            if file_key != reference_key:
                ref_list = list()
                other_list = list()
                uk_list = list()
                for uk in unique_keys:
                    ref_value = reduced_data[uk][reference_key][element_name]
                    other_value = reduced_data[uk][file_key][element_name]
                    ref_list.append(ref_value)
                    other_list.append(other_value)
                    uk_list.append(uk)
                x_coord = ref_list 
                y_coord = other_list
                unique_key_list = uk_list
                fig = plt.figure(figsize=(8, 6))
                ax1 = fig.add_subplot(111)
                ax1.scatter(ref_list, other_list, s=5, c=marker_color[ marker_style_index[file_key] ], marker=marker_style[ marker_style_index[file_key] ], label=file_key)
                ax1.set_title(element_name + ' for: (' + file_key + ' vs. ' + reference_key +')', fontsize=20)
                ax1.set_xlabel('reference value: ' + element_name)
                ax1.set_ylabel(file_key + " value: " + element_name)
                
                #connect the click event to the function
                fig.canvas.mpl_connect('button_press_event', display_point_info)

                plt.show()

    if mode == 2:
        print("not implemented")


    if mode == 3: #make a plain histogram
        for file_key in all_keys:
            if file_key != reference_key:
                delta_list = list()
                for uk in unique_keys:
                    delta = reduced_data[uk][file_key]['delta']
                    delta_list.append(delta)
                fig = plt.figure(figsize=(8, 6))
                ax= fig.add_subplot(111)
                outlier_aware_hist( np.array(delta_list), None, None, nbins=1000 )
                ax.set_title('$\Delta$ ' + element_name + ' for: (' + file_key + ' - ' + reference_key +')', fontsize=20)
                ax.set_ylabel('Number')
                ax.set_xlabel('$\Delta$ ' + element_name)
                plt.show()

    # 
    # for bd in bad_scan_list:
    #     print("===========================")
    #     print("===========================")
    #     for file_key in all_keys:
    #         bdthis = bd
    #         corr = file_key
    #         df = all_df[file_key]
    #         #print(bdthis, bdv, corr)
    #         bd_row = df.loc[ df['scan_name'] == bdthis ]
    #         if len(bd_row) != 0:
    #             print(corr, '&', bd , '&', bd_row.iloc[0]['source'], '&',  round(bd_row.iloc[0]['dtec'],2) , '&', \
    #             bd_row.iloc[0]['quality'], '&', round(bd_row.iloc[0]['snr'],2) , '&', bd_row.iloc[0]['total_mbdelay'])



if __name__ == '__main__':          # official entry point
    main()
    sys.exit(0)
