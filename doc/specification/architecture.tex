
\section{General Architecture}
\label{sec:architecture}


The goal of the re-developed HOPS architecture is provide a component-based design which allows for a greater degree of flexibility
in the types of operations that may be applied to VLBI data during fringe fitting, and avoid the semi-monolithic approach used in the past.
To do this, the software package to be delivered will primarily comprise of a set of loosely coupled libraries which can be composed
into an variety of calibration and fringe-fitting applications. Moreover, rather than attempting to identify and categorize all manner of data
types and data manipulation which may be required by future observations at the outset, a primary goal of this project is to provide a
relatively flexible set of data containers and operators along with a python plugin interface so as to allow for future extensions with minimal revision to the existing code.

For the foreseeable future, the main source of input data to this software will be provided by the DiFX correlator. However, this may not always
be the case, so in order to decouple the correlator from the post-processing software a conversion utility must be provided. This follows
the same paradigm as the current code, where a separate application (difx2mark4, which depends on the DIFXIO library) handles the conversion. However,
in this iteration we propose that this conversion utility operate mainly as a transparent pass-through, merely
converting the correlator output into the native post-processing data format, rather than applying any initial calibration/normalization (e.g. auto-corrs) or
other corrections to the data.

Once the data has been converted to the native post-processing format additional manipulation within the fringe-fitter will take place in several stages, such as normalization, data-flagging, a priori phase/delay and band-pass calibration, and finally fringe search. At intervening stages
both the data objects and data operator objects will be exposed to a python interface and hooks will be provided for external user scripts to 
access and directly modify the data and/or data operators. A extremely simplified diagram of the control flow of such a single-baseline fringe-fitter executable is shown in \ref{fig:fringe-fitter}. A multi-baseline global fringe-fitting algorithm will also be made possible by refactoring the HOPS code, however, the exact nature of this algorithm (e.g. Cotton-Schwab or baseline stacking, etc.) and whether it will best be implemented as a configurable option of a single fringe-fitting executable or as its own separate executable needs to be determine.

Upon completion of fringe fitting, the calibrated data with fringe solution applied will then be saved in a native binary format with the option to convert to an archival format (e.g. HDF5) via a simple conversion utility. Intermediate stages (with partial data corrections applied) may also optionally saved to the native format. It is also desired that
the export of a data flagging table and band-pass correction table be made possible, such an export tool may initially be implemented via user python script. If a multi-package (HOPS/AIPS/CASA) exchange format is adopted/specified then this feature could be made a built-in option in the refactored code.

In addition to the fringe fitter itself, a number of other post-fringing data analysis tools will also be provided. Most important are
a fringe plot utility for diagnostic information about the quality of an individual fringe, as well as the equivalent executables to \textit{alist} and \textit{aedit}, which summarize multiple fringe solutions and display condensed information about multiple scans respectively.
We intended to preserved the existing functionality of these executables, but expect to make the fringe-plot more flexible by providing options
to enable/disable various plotting items.


\begin{figure}[H]
\begin{center}
  \includegraphics[width=0.6\textwidth]{fig/example-single-baseline-fringe-fitter.png}
    \caption{Example of simplified control flow for a single-baseline fringe fitter.}
    \label{fig:fringe-fitter}
\end{center}
\end{figure}




\section{Basic single-baseline fringe-fitting}

In this section we will consider the basic task of fringe-fitting visibility data for a single baseline. Generally speaking this task is much more complicated than we will initially discuss here, but to keep from getting lost in the details we will assume that we have essentially perfect data that only requires a simple delay and delay-rate correction to the correlator's original model. Furthermore we will restrict our discussion to data produced by a single frequency channel, so as to put off the topic of multi-band delay, and assume that both stations are single (upper) side-band and have an ideal flat-top band-pass within the recieving window.

\section{Mathematical Preliminaries}

The fringe-fitter input data from a single-baseline observation consists of the output from the correlator, 
and is referred to as the ``visibilities''. These visibilities are the discrete finite-time (and frequency) cross correlation of the two telescope signals $V_g(t)$ and $ V_h(t)$.

The instantaneous cross correlation of two signals is defined by the integral\footnote{Eq. 3.27 TMS v2}:
\begin{equation}
 r_{gh}(\tau) = \int_{-\infty}^{\infty} \overline{V_h(t-\tau)} V_g(t) dt
\end{equation}
Naturally, in order to facilitate computation this integral must be limited to a finite amount of time, $T$, so we
instead compute:
\begin{equation}
 \hat{r}_{gh}(\tau) = \int_{0}^{T} \overline{ \hat{V}_h(t-\tau)} \hat{V}_g(t) dt
\end{equation}
where the $\hat{V}$ denotes that the signal is zero outside of $0 \leq t < T$. Likewise the time-dependent voltage signals from each of the two telescopes $g$ and $h$ are band-limited
to a finite frequency range (due to filtering and equipment limitations) so we can express the them as the (inverse) Fourier transform of a function of frequency as follows:


and. 
and in practice we
compute:
\begin{equation}
  \hat{r}_{gh}(\tau) = \int_{0}^{T} \overline{V_h(t-\tau)} V_g(t) dt
\end{equation}




\begin{equation}
 V_g(t) = \int_{\infty}^{\infty} g(\nu) e^{i 2\pi \nu t} d\nu = \int_{f_0}^{f_1} [s(\nu) + n_{g}(\nu)] e^{i 2\pi \nu t} d\nu 
\end{equation}
and 
\begin{equation}
  V_h(t) = \int_{\infty}^{\infty} h(\nu) e^{i 2\pi \nu t} d\nu =  \int_{f_0}^{f_1} [s(\nu) + n_{h}(\nu)] e^{i 2\pi \nu t} d\nu 
\end{equation}
Now, since the correlation theorem (a corollary to the more commonly known convolution theorem) states:
\begin{equation}
 \mathcal{F}(p \star q) = \overline{\mathcal{F}(p)} \cdot \mathcal{F}(q) \rightarrow
  p \star q = \mathcal{F}^{-1} \left [ \overline{\mathcal{F}(p)} \cdot \mathcal{F}(q) \right]
\end{equation}
where we denote the cross-correlation operation by $\star$ and pointwise multiplication by the $\cdot$ operator.
We can then re-write the output of the correlator as:
\begin{equation}
 r_{gh}(\tau) = \int_{-\infty}^{\infty}  g(\nu)  \overline{ h(\nu) } e^{i 2\pi \nu \tau} d\nu ,
\end{equation}
which upon inserting our representation of the signal of each station as the sum of astronomical source and system noise, becomes:
\begin{equation}
r_{gh}(\tau) = \int_{f_0}^{f_1} \left[ s(\nu)\overline{s(\nu)} + s(\nu)\overline{n_h(\nu)} + \overline{s(\nu)}n_g(\nu) + n_g(\nu)\overline{n_h(\nu)} \right]  e^{i 2\pi \nu \tau} d\nu
\end{equation}
In practice the output of the correlator is not an instantaneous quantity, but is rather the time average of the above quantity, over 
a pre-determined period of time known as the ``accumulation period'' (AP).:
\begin{multline}
 \frac{1}{(t_1 - t_0)} \int_{t_0}^{t_1} r_{gh}(\tau) dt = \\
 \int_{f_0}^{f_1} \int_{f_0}^{f_1} \left[ s(\nu)\overline{s(\nu)} + s(\nu)\overline{n_h(\nu)} + \overline{s(\nu)}n_g(\nu) + n_g(\nu)\overline{n_h(\nu)} \right]  e^{i 2\pi \nu \tau} d\nu
\end{multline}


However, in practice the output of the correlator is not given primarily as a function the absolute delay $\tau$, 
but rather the residual delay $\Delta \tau$ with respect to the combined effect of the geometric delay $\tau_g$ applied by the correlator (as well as instrumental delays $\tau_i = \tau_0 + \tau_1 + \dots$ accumulated at the stations). For simplicity we
will absorb these (approximately) constant phase nuisance parameters in a pre-factor $\tilde{A}_{gh}$



\begin{equation}
 r_{gh}(\tau) = \tilde{A}_{gh}  \int_{f_0}^{f_1}  g(\nu) \overline{ h(\nu) }  e^{i 2\pi \nu \Delta \tau} d\nu
\end{equation}



\section{The Visibility Data}

As mentioned in the previous section the primary output of the correlator are the ``visibilities'', which are essentially a two-dimensional array of complex values in time and frequency space. However, there are two details which were ignored in the preceeding section which are of importance in treating real data. 

The first detail is that most modern antennas record both electric field polarizations at once, producing four possible cross-correlation ``polarization-products'' for each baseline. Further complicating this is that the signals at each antenna may be recorded in a linear or circularly polarized basis, and so the ``polarization-products'' may be linear-linear, circular-circular or a mixed combination. Clearly delineating the polariation-product associated with the data is important, as a 
time-dependent phase and amplitude modulation can be introduced dpending on the telescope mount type and parallactic angle. We will leave a full discussion of this topic aside for the moment, but suffice to say that our single-baseline data set is extended in an additional ``polarization-product'' dimension.

The second complication is that we almost never deal with a single continuous chunk of bandwidth (i.e. a channel), but often have many channels distributed over a wider range of frequency. This done for several practical reasons. The first reason being that it is difficult to design RF systems that have a perfectly linear phase and amplitude response over a wide bandwidth. However, for narrow channels a linear phase/amplitude response is a perfectly reasonable approximation, and so linear corrections applied in a piecewise manner to several channels can be combined to simulat a flat bandpass over the full frequency range. The second reason (primarily of interest for geodesy) is that for a given recorded bandwidth, a better residual delay estimate can be obtained by spacing the recorded channels further apart in frequency than if they were entirely adjacent. Finally, channelized data
is also useful for RFI mitigation, since a narrow band RFI signal (provided it doesn't entirely saturate the frontend)
confined to a few channels can be easily deleted without having to edit the visibilities directly. While strictly speaking, a ``channel'' is simply shorthand for a frequency offset and thus not truly a separate dimension, for algorithmic considerations it is often convenient to treat this as if it were another independent axis over which the visibilities extend.

Therefore in order to accommodate multiple polarizations and channelization, our single-baseline visibility data generally is most easily represented as a uniform four dimensional array, indexed by polarization-product, channel, time (AP), and frequency (spectral point within a channel). This structure demands that each channel ought to contain the same number of spectral points,
and that each polariation-product ought to contain data for each channel and AP observed. It should be noted that this arrangement does come with some trade-offs between the overall memory usage and in efficiency of indexing across the data array. Ragged arrays would certainly be the most compact memory-wise, however they make random access very inefficient and are unlikely to be the most common use case and so are not considered as a general purpose container. In the occasional instances where channels may not share the same number of spectral points, it is expected that the deficient channels can be padded in order to maintain uniform spacing, or a non-channelized array 
can be used.

